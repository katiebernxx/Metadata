{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Using cached PyMuPDF-1.24.9-cp310-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.9 (from pymupdf)\n",
      "  Using cached PyMuPDFb-1.24.9-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Using cached PyMuPDF-1.24.9-cp310-none-win_amd64.whl (3.2 MB)\n",
      "Using cached PyMuPDFb-1.24.9-py3-none-win_amd64.whl (13.2 MB)\n",
      "Installing collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.9 pymupdf-1.24.9\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\n",
    "#method1\n",
    "# ! pip install fitz\n",
    "# ! pip install pymupdf\n",
    "\n",
    "#method2\n",
    "# ! pip install transformers\n",
    "# ! pip install torch\n",
    "# ! pip install pandas\n",
    "# ! pip install sentencepiece\n",
    "\n",
    "#method3\n",
    "# ! pip install spacy\n",
    "# ! python -m spacy download en_core_web_sm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katie\\anaconda3\\envs\\Ubongo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\katie\\anaconda3\\envs\\Ubongo\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"ex1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4:  Counting the Number of Words per Character in the Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Character  Word Count\n",
      "0          NAMIA         573\n",
      "1           NUZO         730\n",
      "2          DADDY         138\n",
      "3           MAMA           7\n",
      "4       BUBELANG         738\n",
      "5  FEMALE SELLER         113\n"
     ]
    }
   ],
   "source": [
    "## method 1: using regex\n",
    "# we are identifying character names by if they are all caps and on a single line\n",
    "# we do exclude lines with scene descriptors like int and ext to not pick up on setting all caps lines\n",
    "# Problems: the scripts are formatted differently and this does not pick up on the line \"DADDY and MAMA\" for ex because \"and\" is lowercase\n",
    "# Questions: Is this the general format for most scripts? Does it change in different languages?\n",
    "\n",
    "def count_words_per_character(script_text):\n",
    "    lines = script_text.split('\\n')\n",
    "    word_counts = defaultdict(int)\n",
    "    current_characters = []\n",
    "\n",
    "    # Identifying character names as all caps on a single line, including commas and ampersands\n",
    "    pattern = re.compile(r'^[A-Z\\s,&]+$')\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            # Skip common scene description indicators to prevent matching with all caps setting\n",
    "            if any(word in line for word in ['INT', 'EXT', 'DAY', 'NIGHT', 'MORNING', 'AFTERNOON', 'EVENING']):\n",
    "                current_characters = []\n",
    "            else:\n",
    "                # Split the line by commas, ampersands, and ANDs to get individual character names\n",
    "                characters = [char.strip() for char in re.split('[,&]| AND | and ', line)]\n",
    "                current_characters = characters\n",
    "                # if len(current_characters)>1:\n",
    "                #     print(current_characters)\n",
    "        elif current_characters:\n",
    "            # Count words in the dialogue lines\n",
    "            word_count = len(line.split())\n",
    "            for char in current_characters:\n",
    "                # if char == \"MAMA\":\n",
    "                #     print(\"mama\", word_count)\n",
    "                word_counts[char] += word_count\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# Count words per character\n",
    "word_counts = count_words_per_character(pdf_text)\n",
    "\n",
    "df = pd.DataFrame(word_counts.items(), columns=[\"Character\", \"Word Count\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katie\\anaconda3\\envs\\Ubongo\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\katie\\anaconda3\\envs\\Ubongo\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\katie\\.cache\\huggingface\\hub\\models--Davlan--xlm-roberta-base-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# ## method 2: using hugging face named entity recognition to identify proper nouns. Combine with regex method\n",
    "# # problems: model is only trained on the following languages: Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese\n",
    "# # potentially should try testing with scripts in various African languages to see if its name recognition is general enough to work\n",
    "# # problem: model is not doing a great job at identifying character names!\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"Davlan/xlm-roberta-base-ner-hrl\" # 10 language multilingual model\n",
    "# # documentation for model: https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# # Create the NER pipeline\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# # Identify character names using NER\n",
    "# ner_results = nlp(pdf_text)\n",
    "\n",
    "# # Extract character names and filter out non-character entities\n",
    "# character_names = set()\n",
    "# for entity in ner_results:\n",
    "#     if entity['entity'] in {'B-PER', 'I-PER'}:\n",
    "#         character_names.add(entity['word'])\n",
    "\n",
    "# # Debugging: Print identified character names\n",
    "# print(\"Identified Character Names:\", character_names)\n",
    "\n",
    "# # Function to count words per character\n",
    "# def count_words_per_character(script_text, character_names):\n",
    "#     lines = script_text.split('\\n')\n",
    "#     word_counts = defaultdict(int)\n",
    "#     current_characters = []\n",
    "\n",
    "#     # Regex pattern to identify character lines based on script structure\n",
    "#     character_pattern = re.compile(rf\"^({'|'.join(re.escape(name) for name in character_names)})$\", re.IGNORECASE)\n",
    "\n",
    "#     for line in lines:\n",
    "#         line = line.strip()\n",
    "#         if character_pattern.match(line):\n",
    "#             # Debugging: Print matched character lines\n",
    "#             print(\"Matched Character Line:\", line)\n",
    "#             # Split the line by commas, ampersands, and \"and\" to get individual character names\n",
    "#             characters = [char.strip() for char in re.split('[,&]| and ', line, flags=re.IGNORECASE)]\n",
    "#             current_characters = characters\n",
    "#         elif current_characters:\n",
    "#             # Count words in the dialogue lines\n",
    "#             word_count = len(line.split())\n",
    "#             for char in current_characters:\n",
    "#                 word_counts[char] += word_count\n",
    "\n",
    "#     return word_counts\n",
    "\n",
    "# # Count words per character\n",
    "# word_counts = count_words_per_character(pdf_text, character_names)\n",
    "\n",
    "# df = pd.DataFrame(list(word_counts.items()), columns=[\"Character\", \"Word Count\"])\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Character Names: {'RA', 'KO', 'KI', 'KIKOMANDO', 'MA', 'NAMIA'}\n",
      "  Character  Word Count\n",
      "0     NAMIA        2388\n"
     ]
    }
   ],
   "source": [
    "# ## method 3: spacy named entity recognition\n",
    "# # problems: requires testing on whether this will work in other languages, as ner was trained with english\n",
    "# # problem: spacy also seems bad at recognizing these names\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# # Load the spaCy model for English\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Identify character names using spaCy NER\n",
    "# doc = nlp(pdf_text)\n",
    "# character_names = set()\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ == \"PERSON\":\n",
    "#         character_names.add(ent.text.strip())\n",
    "\n",
    "# # Filter to only keep all caps character names\n",
    "# character_names = {name for name in character_names if name.isupper()}\n",
    "\n",
    "# # Debugging: Print identified character names\n",
    "# print(\"Identified Character Names:\", character_names)\n",
    "\n",
    "# # Function to count words per character\n",
    "# def count_words_per_character(script_text, character_names):\n",
    "#     lines = script_text.split('\\n')\n",
    "#     word_counts = defaultdict(int)\n",
    "#     current_characters = []\n",
    "\n",
    "#     # Regex pattern to identify character lines based on script structure\n",
    "#     character_pattern = re.compile(\n",
    "#         rf\"^({'|'.join(re.escape(name) for name in character_names)})([,&]| and )*$\", re.IGNORECASE\n",
    "#     )\n",
    "\n",
    "#     for line in lines:\n",
    "#         line = line.strip()\n",
    "#         if character_pattern.match(line):\n",
    "#             # Split the line by commas, ampersands, and \"and\" to get individual character names\n",
    "#             characters = [char.strip() for char in re.split('[,&]| and ', line, flags=re.IGNORECASE)]\n",
    "#             current_characters = characters\n",
    "#         elif current_characters:\n",
    "#             # Count words in the dialogue lines\n",
    "#             word_count = len(line.split())\n",
    "#             for char in current_characters:\n",
    "#                 word_counts[char] += word_count\n",
    "\n",
    "#     return word_counts\n",
    "\n",
    "# # Count words per character\n",
    "# word_counts = count_words_per_character(pdf_text, character_names)\n",
    "\n",
    "# df = pd.DataFrame(list(word_counts.items()), columns=[\"Character\", \"Word Count\"])\n",
    "# print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubongo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
